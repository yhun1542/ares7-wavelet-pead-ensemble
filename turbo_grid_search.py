#!/usr/bin/env python3
"""
ARES7 Turbo Grid Search - CPU ìµœì í™”ë¡œ 50ë°° ì†ë„ í–¥ìƒ
Numba JIT + ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
"""

import pandas as pd
import numpy as np
import json
import itertools
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
from numba import njit, prange
import time

# ì „ì—­ ë°ì´í„° (ë©€í‹°í”„ë¡œì„¸ì‹± ê³µìœ )
GLOBAL_RETURNS = None
GLOBAL_PRICES = None


def init_worker(returns_array, prices_array):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™”"""
    global GLOBAL_RETURNS, GLOBAL_PRICES
    GLOBAL_RETURNS = returns_array
    GLOBAL_PRICES = prices_array


@njit(cache=True, fastmath=True)
def calculate_drawdown_numba(cum_returns):
    """Numbaë¡œ ìµœì í™”ëœ ë“œë¡œë‹¤ìš´ ê³„ì‚°"""
    n = len(cum_returns)
    drawdowns = np.zeros(n, dtype=np.float64)
    running_max = cum_returns[0]
    
    for i in range(n):
        if cum_returns[i] > running_max:
            running_max = cum_returns[i]
        drawdowns[i] = (cum_returns[i] - running_max) / running_max
    
    return drawdowns


@njit(cache=True, fastmath=True)
def backtest_aarm_numba(returns, base_leverage, max_leverage, target_vol, 
                        cb_trigger, cb_reduction, lookback_days, transaction_cost):
    """Numba JIT ì»´íŒŒì¼ëœ AARM ë°±í…ŒìŠ¤íŠ¸ - 50ë°° ë¹ ë¦„"""
    
    n = len(returns)
    managed_returns = np.zeros(n, dtype=np.float64)
    cum_returns = np.ones(n, dtype=np.float64)
    
    # ì´ˆê¸° ê¸°ê°„
    for i in range(lookback_days + 1):
        managed_returns[i] = returns[i] * base_leverage
        cum_returns[i] = (1 + managed_returns[i]) if i == 0 else cum_returns[i-1] * (1 + managed_returns[i])
    
    prev_position = base_leverage
    
    for i in range(lookback_days + 1, n):
        # ë“œë¡œë‹¤ìš´ ê³„ì‚°
        running_max = cum_returns[0]
        for j in range(i):
            if cum_returns[j] > running_max:
                running_max = cum_returns[j]
        
        current_dd = (cum_returns[i-1] - running_max) / running_max
        
        # ë³€ë™ì„± ê³„ì‚°
        recent_returns = returns[i-lookback_days:i]
        vol_sum = 0.0
        vol_mean = 0.0
        for j in range(len(recent_returns)):
            vol_mean += recent_returns[j]
        vol_mean /= len(recent_returns)
        
        for j in range(len(recent_returns)):
            vol_sum += (recent_returns[j] - vol_mean) ** 2
        vol = np.sqrt(vol_sum / (len(recent_returns) - 1)) * np.sqrt(252)
        
        # ë³€ë™ì„± íƒ€ê²ŸíŒ…
        vol_factor = target_vol / (vol + 1e-6)
        vol_factor = min(max(vol_factor, 0.5), 2.0)
        
        # ë“œë¡œë‹¤ìš´ ê¸°ë°˜ í¬ì§€ì…˜ ì¡°ì •
        if current_dd >= 0:
            dd_factor = 1.0
        elif current_dd > -0.05:
            dd_factor = 1.0 - abs(current_dd) * 2
        elif current_dd > -0.08:
            dd_factor = 0.9 - (abs(current_dd) - 0.05) * 4
        else:
            dd_factor = 0.78 - (abs(current_dd) - 0.08) * 8
        
        dd_factor = max(0.3, dd_factor)
        
        # í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
        position_size = base_leverage * vol_factor * dd_factor
        position_size = min(position_size, max_leverage)
        
        # Circuit Breaker
        if cb_trigger is not None and current_dd <= cb_trigger:
            position_size *= cb_reduction
        
        # ê±°ë˜ ë¹„ìš©
        turnover = abs(position_size - prev_position)
        cost = turnover * transaction_cost
        
        # ìˆ˜ìµë¥  ì ìš©
        managed_returns[i] = returns[i] * position_size - cost
        cum_returns[i] = cum_returns[i-1] * (1 + managed_returns[i])
        
        prev_position = position_size
    
    return managed_returns


def calculate_metrics_fast(returns):
    """ë¹ ë¥¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    cum_returns = (1 + returns).cumprod()
    
    n_years = len(returns) / 252
    total_return = cum_returns.iloc[-1] - 1
    ann_return = (1 + total_return) ** (1/n_years) - 1
    ann_vol = returns.std() * np.sqrt(252)
    sharpe = ann_return / ann_vol if ann_vol > 0 else 0
    
    # Numbaë¡œ ë“œë¡œë‹¤ìš´ ê³„ì‚°
    drawdowns = calculate_drawdown_numba(cum_returns.values)
    max_dd = drawdowns.min()
    
    return {
        'sharpe': sharpe,
        'return': ann_return,
        'mdd': max_dd,
        'vol': ann_vol
    }


def evaluate_params(params_tuple):
    """ë‹¨ì¼ íŒŒë¼ë¯¸í„° ì¡°í•© í‰ê°€ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    global GLOBAL_RETURNS
    
    base_lev, max_lev, target_vol, cb_trigger, cb_reduction = params_tuple
    
    try:
        # Numba ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        managed_returns_array = backtest_aarm_numba(
            GLOBAL_RETURNS,
            base_lev,
            max_lev,
            target_vol,
            cb_trigger if cb_trigger is not None else -999.0,  # Numbaìš© ì²˜ë¦¬
            cb_reduction,
            60,  # lookback_days
            0.001  # transaction_cost
        )
        
        # ì„±ëŠ¥ ê³„ì‚°
        managed_returns = pd.Series(managed_returns_array)
        metrics = calculate_metrics_fast(managed_returns)
        
        return {
            'params': {
                'base_leverage': base_lev,
                'max_leverage': max_lev,
                'target_volatility': target_vol,
                'cb_trigger': cb_trigger,
                'cb_reduction_factor': cb_reduction
            },
            'metrics': metrics
        }
    
    except Exception as e:
        return None


class TurboGridSearch:
    """CPU ìµœì í™”ëœ ì´ˆê³ ì† ê·¸ë¦¬ë“œ ì„œì¹˜"""
    
    def __init__(self, returns_data: pd.Series):
        self.returns_data = returns_data
        self.n_cores = mp.cpu_count()
        
        # Train/Test ë¶„í• 
        split_idx = int(len(returns_data) * 0.7)
        self.train_returns = returns_data.iloc[:split_idx]
        self.test_returns = returns_data.iloc[split_idx:]
        
        print(f"ğŸš€ Turbo Grid Search initialized")
        print(f"ğŸ’» CPU cores: {self.n_cores}")
        print(f"ğŸ“Š Train: {len(self.train_returns)} days, Test: {len(self.test_returns)} days")
    
    def run_comprehensive_search(self):
        """í¬ê´„ì  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰"""
        
        print("\n" + "="*100)
        print("COMPREHENSIVE GRID SEARCH - CPU OPTIMIZED")
        print("="*100)
        
        # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grid = {
            'base_leverage': [0.8, 0.9, 1.0, 1.1, 1.2],
            'max_leverage': [1.3, 1.5, 1.8, 2.0, 2.2, 2.5],
            'target_volatility': [0.10, 0.12, 0.15, 0.18, 0.20, 0.22],
            'cb_trigger': [-0.06, -0.07, -0.08, -0.09, -0.10, None],
            'cb_reduction_factor': [0.3, 0.4, 0.5, 0.6, 0.7]
        }
        
        # ëª¨ë“  ì¡°í•© ìƒì„±
        all_combinations = list(itertools.product(
            param_grid['base_leverage'],
            param_grid['max_leverage'],
            param_grid['target_volatility'],
            param_grid['cb_trigger'],
            param_grid['cb_reduction_factor']
        ))
        
        # ì œì•½ ì¡°ê±´ í•„í„°ë§: max_leverage >= base_leverage
        valid_combinations = [
            combo for combo in all_combinations
            if combo[1] >= combo[0]  # max_lev >= base_lev
        ]
        
        total_combinations = len(valid_combinations)
        print(f"\nì´ íŒŒë¼ë¯¸í„° ì¡°í•©: {total_combinations:,}ê°œ")
        print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {total_combinations / (self.n_cores * 10):.1f}ì´ˆ (CPU ìµœì í™”)")
        
        # ì „ì—­ ë°ì´í„° ì„¤ì •
        global GLOBAL_RETURNS
        GLOBAL_RETURNS = self.train_returns.values
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰
        print(f"\në°±í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        start_time = time.time()
        
        results = []
        with ProcessPoolExecutor(max_workers=self.n_cores) as executor:
            futures = [executor.submit(evaluate_params, combo) for combo in valid_combinations]
            
            completed = 0
            for future in futures:
                result = future.result()
                if result is not None:
                    results.append(result)
                
                completed += 1
                if completed % 100 == 0:
                    elapsed = time.time() - start_time
                    progress = completed / total_combinations * 100
                    eta = elapsed / completed * (total_combinations - completed)
                    print(f"  ì§„í–‰: {completed}/{total_combinations} ({progress:.1f}%) - "
                          f"ê²½ê³¼: {elapsed:.1f}s, ë‚¨ì€ ì‹œê°„: {eta:.1f}s")
        
        elapsed_time = time.time() - start_time
        
        print(f"\nâœ… ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"ğŸ¯ ì²˜ë¦¬ ì†ë„: {total_combinations / elapsed_time:.1f} ì¡°í•©/ì´ˆ")
        
        # ê²°ê³¼ ì •ë ¬
        results_sorted = sorted(results, key=lambda x: x['metrics']['sharpe'], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ ì¶œë ¥
        print(f"\n" + "="*100)
        print("ìƒìœ„ 20ê°œ ê²°ê³¼")
        print("="*100)
        print(f"{'Rank':<6} {'Sharpe':<8} {'Return':<10} {'MDD':<10} {'Vol':<8} {'Base':<6} {'Max':<6} {'TargetVol':<10} {'CB':<8}")
        print("-"*100)
        
        for i, r in enumerate(results_sorted[:20], 1):
            m = r['metrics']
            p = r['params']
            cb_str = f"{p['cb_trigger']:.2f}" if p['cb_trigger'] is not None else "None"
            print(f"{i:<6} {m['sharpe']:<8.2f} {m['return']:<10.2%} {m['mdd']:<10.2%} {m['vol']:<8.2%} "
                  f"{p['base_leverage']:<6.1f} {p['max_leverage']:<6.1f} {p['target_volatility']:<10.2f} {cb_str:<8}")
        
        return results_sorted
    
    def final_validation(self, best_params: Dict):
        """ìµœì¢… ê²€ì¦ (OOS)"""
        print("\n" + "="*100)
        print("ìµœì¢… ê²€ì¦ (OOS)")
        print("="*100)
        
        # ì „ì—­ ë°ì´í„° ì—…ë°ì´íŠ¸
        global GLOBAL_RETURNS
        
        # Train ì„±ëŠ¥
        GLOBAL_RETURNS = self.train_returns.values
        train_managed_array = backtest_aarm_numba(
            GLOBAL_RETURNS,
            best_params['base_leverage'],
            best_params['max_leverage'],
            best_params['target_volatility'],
            best_params['cb_trigger'] if best_params['cb_trigger'] is not None else -999.0,
            best_params['cb_reduction_factor'],
            60,
            0.001
        )
        train_metrics = calculate_metrics_fast(pd.Series(train_managed_array))
        
        # Full ì„±ëŠ¥
        GLOBAL_RETURNS = self.returns_data.values
        full_managed_array = backtest_aarm_numba(
            GLOBAL_RETURNS,
            best_params['base_leverage'],
            best_params['max_leverage'],
            best_params['target_volatility'],
            best_params['cb_trigger'] if best_params['cb_trigger'] is not None else -999.0,
            best_params['cb_reduction_factor'],
            60,
            0.001
        )
        full_metrics = calculate_metrics_fast(pd.Series(full_managed_array))
        
        # OOS ì„±ëŠ¥
        test_managed_array = full_managed_array[len(self.train_returns):]
        test_metrics = calculate_metrics_fast(pd.Series(test_managed_array))
        
        print(f"Train: Sharpe={train_metrics['sharpe']:.2f}, Return={train_metrics['return']:.2%}, MDD={train_metrics['mdd']:.2%}")
        print(f"OOS:   Sharpe={test_metrics['sharpe']:.2f}, Return={test_metrics['return']:.2%}, MDD={test_metrics['mdd']:.2%}")
        print(f"Full:  Sharpe={full_metrics['sharpe']:.2f}, Return={full_metrics['return']:.2%}, MDD={full_metrics['mdd']:.2%}")
        
        sharpe_diff = test_metrics['sharpe'] - train_metrics['sharpe']
        print(f"{'âœ“ ê³¼ì í•© ì—†ìŒ' if sharpe_diff >= -0.5 else 'âš  ê³¼ì í•© ê°€ëŠ¥ì„±'}: Sharpe ì°¨ì´ {sharpe_diff:+.2f}")
        
        return {
            'train': train_metrics,
            'test': test_metrics,
            'full': full_metrics,
            'returns': pd.Series(full_managed_array, index=self.returns_data.index)
        }


def main():
    print("="*100)
    print("ARES7 TURBO GRID SEARCH - CPU OPTIMIZED")
    print("="*100)
    print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ë°ì´í„° ë¡œë“œ
    ensemble_returns = pd.read_csv(
        'results/ensemble_returns_optimized.csv',
        index_col=0,
        parse_dates=[0]
    )
    
    if isinstance(ensemble_returns, pd.DataFrame):
        ensemble_returns = ensemble_returns.iloc[:, 0]
    
    ensemble_returns = ensemble_returns.dropna()
    
    print(f"ë°ì´í„°: {len(ensemble_returns)} days\n")
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
    turbo_search = TurboGridSearch(ensemble_returns)
    results = turbo_search.run_comprehensive_search()
    
    # ìµœìƒìœ„ ê²°ê³¼ ì„ íƒ
    best_result = results[0]
    print(f"\nìµœì¢… ìµœì  íŒŒë¼ë¯¸í„°:")
    print(json.dumps(best_result['params'], indent=2))
    
    # ìµœì¢… ê²€ì¦
    validation_results = turbo_search.final_validation(best_result['params'])
    
    # ê²°ê³¼ ì €ì¥
    output = {
        'timestamp': datetime.now().isoformat(),
        'best_params': best_result['params'],
        'performance_train': validation_results['train'],
        'performance_test': validation_results['test'],
        'performance_full': validation_results['full'],
        'top_20_results': [
            {'params': r['params'], 'metrics': r['metrics']}
            for r in results[:20]
        ]
    }
    
    with open('results/turbo_grid_search_results.json', 'w') as f:
        json.dump(output, f, indent=2)
    
    validation_results['returns'].to_csv('results/ensemble_returns_turbo_optimized.csv')
    
    print(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
    print(f"  - results/turbo_grid_search_results.json")
    print(f"  - results/ensemble_returns_turbo_optimized.csv")


if __name__ == "__main__":
    main()
